{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, Adagrad, RMSprop\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Data Generators to make Data Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 105\n",
    "IMG_SIZE = (SIZE, SIZE)\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Define training augmentations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2)\n",
    "\n",
    "\n",
    "# Define testing augmentations\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create example augmented images\n",
    "# img = load_img('F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Training/All Cases/Correct/Correct/IMG_1500.jpg')\n",
    "# x = img_to_array(img)\n",
    "# x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# i = 0\n",
    "# for batch in train_datagen.flow(x, batch_size=1, save_to_dir='F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/preview', save_format='jpeg'):\n",
    "#     i+=1\n",
    "#     if i > 9:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(base_dir, gen_type):\n",
    "    if gen_type == 'train':\n",
    "        gen1 = train_datagen.flow_from_directory(base_dir+'/Greek', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=1)\n",
    "        gen2 = train_datagen.flow_from_directory(base_dir+'/Both', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=2)\n",
    "    else:\n",
    "        gen1 = test_datagen.flow_from_directory(base_dir+'/Greek', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=3)\n",
    "        gen2 = test_datagen.flow_from_directory(base_dir+'/Both', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=4)\n",
    "    \n",
    "    while True:\n",
    "        input1 = gen1.next()\n",
    "        input2 = gen2.next()\n",
    "        yield([input1[0], input2[0]], input1[1]==input2[1])\n",
    "    \n",
    "train_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/omniglot/train_snn'\n",
    "val_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/omniglot/val_snn/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y, preds, margin=1):\n",
    "    y = tf.cast(y, preds.dtype)\n",
    "    \n",
    "    squaredPreds = K.square(preds)\n",
    "    squaredMargin = K.square(K.maximum(margin - preds, 0))\n",
    "    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Siamese Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    # Input layers for each network twin\n",
    "    input1 = Input(input_shape)\n",
    "    input2 = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (5,5), input_shape=(SIZE, SIZE, NUM_CHANNELS), data_format='channels_last'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "# #     model.add(Dropout(0.5))\n",
    "#     model.add(MaxPooling2D())\n",
    "\n",
    "#     model.add(Conv2D(64, (3,3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "# #     model.add(Dropout(0.5))\n",
    "#     model.add(MaxPooling2D())\n",
    "\n",
    "#     model.add(Conv2D(128, (3,3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "#     model.add(Dropout(0.5))\n",
    "    \n",
    "#     model.add(Dense(64))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "#     model.add(Dropout(0.5))\n",
    "    \n",
    "\n",
    "    model = VGG16(include_top=False, input_shape=(SIZE, SIZE, NUM_CHANNELS))\n",
    "\n",
    "    # Only retrain last block of VGG16 - 3 conv layers\n",
    "    for layer in model.layers:\n",
    "#         if not layer.name.startswith('block5'):\n",
    "        layer.trainable = False\n",
    "            \n",
    "    # Add own layers at end for task\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    dense1 = Dense(1024, activation='relu')(flat1)#, kernel_regularizer=l2(1E-2), bias_regularizer=l2(1E-2))(flat1)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    \n",
    "    model = Model(inputs=model.inputs, outputs=dropout1)\n",
    "\n",
    "    \n",
    "    # Create features for each input image for comparison\n",
    "    feature1 = model(input1)\n",
    "    feature2 = model(input2)\n",
    "    \n",
    "    # Custom layer to compute the L1 distance between the features of the two images\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([feature1, feature2])\n",
    "    \n",
    "    # Calculate similarity score\n",
    "    similarity = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[input1, input2], outputs=similarity)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_5 (Functional)       (None, 1024)         19434304    input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1024)         0           functional_5[0][0]               \n",
      "                                                                 functional_5[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            1025        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 19,435,329\n",
      "Trainable params: 4,720,641\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model((IMG_SIZE[0], IMG_SIZE[1], NUM_CHANNELS))\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr = 0.00003)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_logdir = \"logs/scalars/\"\n",
    "for f in os.listdir(base_logdir):\n",
    "    file_path = os.path.join(base_logdir, f)\n",
    "    \n",
    "    shutil.rmtree(file_path)\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 1 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      " 2/12 [====>.........................] - ETA: 2s - loss: 0.7413 - accuracy: 0.5625 - precision: 0.6923 - recall: 0.4737WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0710s vs `on_train_batch_end` time: 0.5221s). Check your callbacks.\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7486 - accuracy: 0.5208 - precision: 0.5114 - recall: 0.4787Found 96 images belonging to 1 classes.\n",
      "Found 96 images belonging to 2 classes.\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 0.7486 - accuracy: 0.5208 - precision: 0.5114 - recall: 0.4787 - val_loss: 0.6928 - val_accuracy: 0.4896 - val_precision: 0.4909 - val_recall: 0.5625\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.6851 - accuracy: 0.5870 - precision: 0.5943 - recall: 0.6562 - val_loss: 0.7050 - val_accuracy: 0.4792 - val_precision: 0.4844 - val_recall: 0.6458\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6557 - accuracy: 0.5761 - precision: 0.5667 - recall: 0.5667 - val_loss: 0.6726 - val_accuracy: 0.6250 - val_precision: 0.5714 - val_recall: 0.6512\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.7382 - accuracy: 0.5380 - precision: 0.5474 - recall: 0.5532 - val_loss: 0.6756 - val_accuracy: 0.5938 - val_precision: 0.6066 - val_recall: 0.7115\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6606 - accuracy: 0.5978 - precision: 0.5732 - recall: 0.5465 - val_loss: 0.6467 - val_accuracy: 0.6979 - val_precision: 0.7447 - val_recall: 0.6731\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6656 - accuracy: 0.6359 - precision: 0.6465 - recall: 0.6667 - val_loss: 0.6672 - val_accuracy: 0.5729 - val_precision: 0.5849 - val_recall: 0.6200\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.6687 - accuracy: 0.5924 - precision: 0.6211 - recall: 0.6020 - val_loss: 0.6465 - val_accuracy: 0.6562 - val_precision: 0.6190 - val_recall: 0.8125\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6452 - accuracy: 0.6033 - precision: 0.5700 - recall: 0.6552 - val_loss: 0.6430 - val_accuracy: 0.6562 - val_precision: 0.7692 - val_recall: 0.4255\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6382 - accuracy: 0.6467 - precision: 0.7000 - recall: 0.5269 - val_loss: 0.6252 - val_accuracy: 0.6875 - val_precision: 0.6809 - val_recall: 0.6809\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6720 - accuracy: 0.6522 - precision: 0.6161 - recall: 0.7667 - val_loss: 0.6456 - val_accuracy: 0.6354 - val_precision: 0.6905 - val_recall: 0.5686\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6374 - accuracy: 0.6576 - precision: 0.6667 - recall: 0.5843 - val_loss: 0.6529 - val_accuracy: 0.6979 - val_precision: 0.7568 - val_recall: 0.5833\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6339 - accuracy: 0.6087 - precision: 0.6477 - recall: 0.5816 - val_loss: 0.6331 - val_accuracy: 0.6354 - val_precision: 0.6458 - val_recall: 0.6327\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6140 - accuracy: 0.7120 - precision: 0.6837 - recall: 0.7528 - val_loss: 0.6258 - val_accuracy: 0.6771 - val_precision: 0.6889 - val_recall: 0.6458\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.5835 - accuracy: 0.7031 - precision: 0.7097 - recall: 0.6875 - val_loss: 0.5784 - val_accuracy: 0.8125 - val_precision: 0.8043 - val_recall: 0.8043\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.6316 - accuracy: 0.6467 - precision: 0.6374 - recall: 0.6444 - val_loss: 0.6203 - val_accuracy: 0.7188 - val_precision: 0.7000 - val_recall: 0.6512\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.5531 - accuracy: 0.7283 - precision: 0.7048 - recall: 0.7957 - val_loss: 0.6206 - val_accuracy: 0.7188 - val_precision: 0.7568 - val_recall: 0.6087\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.5394 - accuracy: 0.7283 - precision: 0.7467 - recall: 0.6437 - val_loss: 0.6194 - val_accuracy: 0.6562 - val_precision: 0.7931 - val_recall: 0.4600\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.5696 - accuracy: 0.7011 - precision: 0.7156 - recall: 0.7647 - val_loss: 0.6221 - val_accuracy: 0.6875 - val_precision: 0.7174 - val_recall: 0.6600\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 0.5818 - accuracy: 0.6957 - precision: 0.6837 - recall: 0.7283 - val_loss: 0.6295 - val_accuracy: 0.6562 - val_precision: 0.7273 - val_recall: 0.5000\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.5845 - accuracy: 0.6793 - precision: 0.6593 - recall: 0.6818 - val_loss: 0.6216 - val_accuracy: 0.7083 - val_precision: 0.8056 - val_recall: 0.5800\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.5610 - accuracy: 0.7011 - precision: 0.7024 - recall: 0.6629 - val_loss: 0.6194 - val_accuracy: 0.6979 - val_precision: 0.7111 - val_recall: 0.6667\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.5310 - accuracy: 0.7609 - precision: 0.7579 - recall: 0.7742 - val_loss: 0.6128 - val_accuracy: 0.7292 - val_precision: 0.8108 - val_recall: 0.6122\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.5784 - accuracy: 0.6630 - precision: 0.6400 - recall: 0.7111 - val_loss: 0.6195 - val_accuracy: 0.6667 - val_precision: 0.7097 - val_recall: 0.4889\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.5274 - accuracy: 0.7554 - precision: 0.7624 - recall: 0.7857 - val_loss: 0.6057 - val_accuracy: 0.6875 - val_precision: 0.7568 - val_recall: 0.5714\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4953 - accuracy: 0.7446 - precision: 0.7087 - recall: 0.8111 - val_loss: 0.5893 - val_accuracy: 0.7188 - val_precision: 0.7838 - val_recall: 0.6042\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4922 - accuracy: 0.7826 - precision: 0.7889 - recall: 0.7717 - val_loss: 0.5920 - val_accuracy: 0.7188 - val_precision: 0.7674 - val_recall: 0.6600\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.5228 - accuracy: 0.7448 - precision: 0.7238 - recall: 0.7917 - val_loss: 0.6061 - val_accuracy: 0.6979 - val_precision: 0.8000 - val_recall: 0.6038\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.5914 - accuracy: 0.6739 - precision: 0.6915 - recall: 0.6771 - val_loss: 0.5822 - val_accuracy: 0.6771 - val_precision: 0.6562 - val_recall: 0.5122\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.5080 - accuracy: 0.7500 - precision: 0.7500 - recall: 0.7333 - val_loss: 0.5971 - val_accuracy: 0.6875 - val_precision: 0.7561 - val_recall: 0.6078\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.5430 - accuracy: 0.7011 - precision: 0.6818 - recall: 0.7895 - val_loss: 0.6018 - val_accuracy: 0.7396 - val_precision: 0.8571 - val_recall: 0.6000\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.5155 - accuracy: 0.7120 - precision: 0.6786 - recall: 0.6867 - val_loss: 0.5847 - val_accuracy: 0.7812 - val_precision: 0.8649 - val_recall: 0.6667\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.5287 - accuracy: 0.7174 - precision: 0.7059 - recall: 0.7660 - val_loss: 0.5624 - val_accuracy: 0.7500 - val_precision: 0.7949 - val_recall: 0.6596\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.5210 - accuracy: 0.7337 - precision: 0.7447 - recall: 0.7368 - val_loss: 0.5852 - val_accuracy: 0.7292 - val_precision: 0.7500 - val_recall: 0.6522\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.5261 - accuracy: 0.7609 - precision: 0.7308 - recall: 0.8261 - val_loss: 0.5853 - val_accuracy: 0.7188 - val_precision: 0.8529 - val_recall: 0.5686\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 90ms/step - loss: 0.5571 - accuracy: 0.7446 - precision: 0.7097 - recall: 0.7674 - val_loss: 0.5727 - val_accuracy: 0.7812 - val_precision: 0.8529 - val_recall: 0.6444\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.4764 - accuracy: 0.7717 - precision: 0.7895 - recall: 0.7732 - val_loss: 0.6038 - val_accuracy: 0.7083 - val_precision: 0.7561 - val_recall: 0.6327\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.5134 - accuracy: 0.7391 - precision: 0.6957 - recall: 0.8602 - val_loss: 0.6101 - val_accuracy: 0.6979 - val_precision: 0.8065 - val_recall: 0.5208\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.4907 - accuracy: 0.7663 - precision: 0.7553 - recall: 0.7802 - val_loss: 0.6132 - val_accuracy: 0.6562 - val_precision: 0.7667 - val_recall: 0.4694\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.4783 - accuracy: 0.7663 - precision: 0.7882 - recall: 0.7283 - val_loss: 0.5735 - val_accuracy: 0.7396 - val_precision: 0.9286 - val_recall: 0.5306\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.4919 - accuracy: 0.7812 - precision: 0.7670 - recall: 0.8144 - val_loss: 0.5538 - val_accuracy: 0.7917 - val_precision: 0.8000 - val_recall: 0.6829\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.4833 - accuracy: 0.7663 - precision: 0.7379 - recall: 0.8261 - val_loss: 0.6040 - val_accuracy: 0.7292 - val_precision: 0.9167 - val_recall: 0.5893\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.5305 - accuracy: 0.7120 - precision: 0.6875 - recall: 0.7416 - val_loss: 0.5865 - val_accuracy: 0.7188 - val_precision: 0.7647 - val_recall: 0.5778\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.4359 - accuracy: 0.8261 - precision: 0.7921 - recall: 0.8791 - val_loss: 0.5675 - val_accuracy: 0.7604 - val_precision: 0.8049 - val_recall: 0.6875\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.4697 - accuracy: 0.8152 - precision: 0.8265 - recall: 0.8265 - val_loss: 0.5660 - val_accuracy: 0.7604 - val_precision: 0.7727 - val_recall: 0.7234\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4849 - accuracy: 0.7826 - precision: 0.7826 - recall: 0.7826 - val_loss: 0.5832 - val_accuracy: 0.6667 - val_precision: 0.6786 - val_recall: 0.4524\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4727 - accuracy: 0.7554 - precision: 0.7579 - recall: 0.7660 - val_loss: 0.5737 - val_accuracy: 0.7812 - val_precision: 0.8571 - val_recall: 0.6522\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.4548 - accuracy: 0.7880 - precision: 0.7527 - recall: 0.8140 - val_loss: 0.6038 - val_accuracy: 0.7083 - val_precision: 0.7619 - val_recall: 0.6400\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.4667 - accuracy: 0.8261 - precision: 0.8119 - recall: 0.8632 - val_loss: 0.5568 - val_accuracy: 0.7188 - val_precision: 0.7500 - val_recall: 0.6383\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.5069 - accuracy: 0.7609 - precision: 0.7547 - recall: 0.8163 - val_loss: 0.5397 - val_accuracy: 0.7708 - val_precision: 0.8611 - val_recall: 0.6458\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4509 - accuracy: 0.7826 - precision: 0.7368 - recall: 0.8235 - val_loss: 0.5413 - val_accuracy: 0.7917 - val_precision: 0.9062 - val_recall: 0.6304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d270c9f358>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generator(train_dir, 'train'), steps_per_epoch=200//BATCH_SIZE, \n",
    "          validation_data=generator(val_dir, 'val'), validation_steps=96//BATCH_SIZE,\n",
    "#           class_weight={0: 2, 1: 1}, \n",
    "          epochs=50, callbacks=[tensorboard_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2620), started 0:05:13 ago. (Use '!kill 2620' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-47a917d354e13a81\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-47a917d354e13a81\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/scalars --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir2 = val_dir + '/Both'\n",
    "im_correct = []\n",
    "im_incorrect = []\n",
    "\n",
    "# Load in validation data\n",
    "for folder in os.listdir(val_dir2):\n",
    "    if folder == 'Greek':\n",
    "        folder_path = os.path.join(val_dir2, folder)\n",
    "        for im in os.listdir(folder_path):\n",
    "            im_array = cv2.imread(os.path.join(folder_path, im))\n",
    "            im_array = cv2.resize(im_array, IMG_SIZE)\n",
    "            im_array = cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB)\n",
    "            im_array = im_array/255\n",
    "            im_correct.append(im_array)\n",
    "    else:\n",
    "        folder_path = os.path.join(val_dir2, folder)\n",
    "        for im in os.listdir(folder_path):\n",
    "            im_array = cv2.imread(os.path.join(folder_path, im))\n",
    "            im_array = cv2.resize(im_array, IMG_SIZE)\n",
    "            im_array = cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB)\n",
    "            im_array = im_array/255\n",
    "            im_incorrect.append(im_array)\n",
    "            \n",
    "im_correct = np.asarray(im_correct)\n",
    "im_incorrect = np.asarray(im_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_correct = 20  # Number of correct images to compare against\n",
    "\n",
    "predict_correct = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for im in im_correct:\n",
    "    Y = []\n",
    "    idxs = np.random.randint(im_correct.shape[0], size=n_correct)  # Correct images to compare against\n",
    "    \n",
    "    for idx in idxs:\n",
    "        im_ref = im_correct[idx]\n",
    "        im_ref = im_ref.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        im = im.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        y = np.rint(model.predict([im_ref, im]))\n",
    "        Y.append(y)\n",
    "     \n",
    "    vote_same = np.sum(Y)\n",
    "    # If more than half the images compared to get voted as same, then set vote to same, otherwise to different\n",
    "    if vote_same >= n_correct/2:\n",
    "        Y_vote = 1\n",
    "        predict_correct += 1\n",
    "    else:\n",
    "        Y_vote = 0\n",
    "        fn += 1\n",
    "        \n",
    "\n",
    "for im in im_incorrect:\n",
    "    Y = []\n",
    "    idxs = np.random.randint(im_correct.shape[0], size=n_correct)  # Correct images to compare against\n",
    "    \n",
    "    for idx in idxs:\n",
    "        im_ref = im_correct[idx]\n",
    "        im_ref = im_ref.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        im = im.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        y = np.rint(model.predict([im_ref, im]))\n",
    "        Y.append(y)\n",
    "     \n",
    "    vote_same = np.sum(Y)\n",
    "    # If more than half the images compared to get voted as same, then set vote to same, otherwise to different\n",
    "    if vote_same >= n_correct/2:\n",
    "        Y_vote = 1\n",
    "        fp += 1\n",
    "    else:\n",
    "        Y_vote = 0\n",
    "        predict_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy =  0.8229166666666666\n",
      "Validation Precision =  0.9294117647058824\n",
      "Validation Recall =  0.8777777777777778\n"
     ]
    }
   ],
   "source": [
    "Val_acc = predict_correct/96\n",
    "print(\"Validation Accuracy = \", Val_acc)\n",
    "\n",
    "Val_precision = predict_correct/(predict_correct + fp)\n",
    "print(\"Validation Precision = \", Val_precision)\n",
    "\n",
    "Val_recall = predict_correct/(predict_correct + fn)\n",
    "print(\"Validation Recall = \", Val_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-way validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task(N, source):\n",
    "    im_list = []\n",
    "    target_list = []\n",
    "    if source == 'train':\n",
    "        im_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Training/All Cases/Both'\n",
    "    else:\n",
    "        im_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Validation/All Cases/Both'\n",
    "    \n",
    "    # Get single reference image\n",
    "    reference = rng.choice(os.listdir(im_dir+'/Correct'))\n",
    "    im_reference = cv2.imread(os.path.join(im_dir+'/Correct', reference))\n",
    "    im_reference = cv2.resize(im_reference, IMG_SIZE)\n",
    "    im_reference = cv2.cvtColor(im_reference, cv2.COLOR_BGR2RGB)\n",
    "    im_reference = im_reference/255\n",
    "    im_reference = im_reference.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "    target_reference = 0\n",
    "        \n",
    "    #plt.imshow(im_reference)\n",
    "    #plt.show()\n",
    "        \n",
    "    # Get N test images\n",
    "    for i in range(N):\n",
    "        if i == 0:\n",
    "            test = rng.choice(os.listdir(im_dir+'/Correct'))\n",
    "            im_test = cv2.imread(os.path.join(im_dir+'/Correct', test))\n",
    "        else:\n",
    "            test = rng.choice(os.listdir(im_dir+'/Incorrect'))\n",
    "            im_test = cv2.imread(os.path.join(im_dir+'/Incorrect', test))\n",
    "\n",
    "        im_test = cv2.resize(im_test, IMG_SIZE)\n",
    "        im_test = cv2.cvtColor(im_test, cv2.COLOR_BGR2RGB)\n",
    "        im_test = im_test/255\n",
    "        im_list.append(im_test)\n",
    "        if i == 0:\n",
    "            target_list.append(0)\n",
    "        else:\n",
    "            target_list.append(1)\n",
    "        \n",
    "    #plt.imshow(im_list[-1])\n",
    "    #plt.show()\n",
    "    \n",
    "    #im_list, target_list = shuffle(im_list, target_list)\n",
    "    \n",
    "    im_array = np.asarray(im_list)\n",
    "    \n",
    "    return im_reference, target_reference, im_array, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_way_validate(model, N, it, source):\n",
    "    n_correct = 0\n",
    "    \n",
    "    for i in range(it):\n",
    "        score_list = []\n",
    "        im_reference, target_reference, im_list, target_list = create_task(N, source)\n",
    "        \n",
    "        for j in range(N):\n",
    "            score = model.predict([im_reference, np.reshape(im_list[j], (1,SIZE,SIZE,NUM_CHANNELS))])\n",
    "            score_list.append(score)\n",
    "            \n",
    "        if np.argmax(score_list) == np.argmin(target_list):\n",
    "            n_correct += 1\n",
    "            \n",
    "    percent_correct = (100*n_correct / it)\n",
    "    \n",
    "    return percent_correct, score_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.5\n"
     ]
    }
   ],
   "source": [
    "N_way = 10\n",
    "iterations = 40\n",
    "val_source = 'train'\n",
    "\n",
    "result, score_list = n_way_validate(model, N_way, iterations, val_source)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5\n"
     ]
    }
   ],
   "source": [
    "val_source = 'validation'\n",
    "\n",
    "result, score_list = n_way_validate(model, N_way, iterations, val_source)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
