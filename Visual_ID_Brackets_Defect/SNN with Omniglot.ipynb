{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, Adagrad, RMSprop\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Data Generators to make Data Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "IMG_SIZE = (SIZE, SIZE)\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Define training augmentations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    #zca_whitening=True,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "# Define testing augmentations\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create example augmented images\n",
    "# img = load_img('F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Training/All Cases/Correct/Correct/IMG_1500.jpg')\n",
    "# x = img_to_array(img)\n",
    "# x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# i = 0\n",
    "# for batch in train_datagen.flow(x, batch_size=1, save_to_dir='F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/preview', save_format='jpeg'):\n",
    "#     i+=1\n",
    "#     if i > 9:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(base_dir, gen_type):\n",
    "    if gen_type == 'train':\n",
    "        gen1 = train_datagen.flow_from_directory(base_dir+'/Correct', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=1)\n",
    "        gen2 = train_datagen.flow_from_directory(base_dir+'/Both', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=2)\n",
    "    else:\n",
    "        gen1 = test_datagen.flow_from_directory(base_dir+'/Correct', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=3)\n",
    "        gen2 = test_datagen.flow_from_directory(base_dir+'/Both', target_size=IMG_SIZE, color_mode='rgb', batch_size=BATCH_SIZE, class_mode='binary', shuffle=True, seed=4)\n",
    "    \n",
    "    while True:\n",
    "        input1 = gen1.next()\n",
    "        input2 = gen2.next()\n",
    "        yield([input1[0], input2[0]], input1[1]==input2[1])  ### TEST CLASS LABEL FOR SINGLE CLASS GENERATOR - SHOULD BE 0\n",
    "    \n",
    "\n",
    "# train_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Training/All Cases'\n",
    "train_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Training/Edge Cases'\n",
    "# val_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Validation/All Cases'\n",
    "val_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Validation/Edge Cases'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y, preds, margin=1):\n",
    "    y = tf.cast(y, preds.dtype)\n",
    "    \n",
    "    squaredPreds = K.square(preds)\n",
    "    squaredMargin = K.square(K.maximum(margin - preds, 0))\n",
    "    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Siamese Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    # Input layers for each network twin\n",
    "    input1 = Input(input_shape)\n",
    "    input2 = Input(input_shape)\n",
    "    \n",
    "#     # Convolutional Neural Network\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(16, (3,3), input_shape=(SIZE, SIZE, NUM_CHANNELS), data_format='channels_last'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(MaxPooling2D())\n",
    "\n",
    "#     model.add(Conv2D(32, (3,3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(MaxPooling2D())\n",
    "\n",
    "#     model.add(Conv2D(32, (3,3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(MaxPooling2D())\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(1024))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.Activation(activations.relu))\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "\n",
    "    model = VGG16(include_top=False, input_shape=(SIZE, SIZE, NUM_CHANNELS))\n",
    "\n",
    "    # Only retrain last block of VGG16 - 3 conv layers\n",
    "    for layer in model.layers:\n",
    "#         if not layer.name.startswith('block5'):\n",
    "        layer.trainable = False\n",
    "            \n",
    "    # Add own layers at end for task\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    dense1 = Dense(1024, activation='relu')(flat1)#, kernel_regularizer=l2(1E-2), bias_regularizer=l2(1E-2))(flat1)\n",
    "    dropout1 = Dropout(0.3)(dense1)\n",
    "    \n",
    "    model = Model(inputs=model.inputs, outputs=dropout1)\n",
    "\n",
    "    \n",
    "    # Create features for each input image for comparison\n",
    "    feature1 = model(input1)\n",
    "    feature2 = model(input2)\n",
    "    \n",
    "    # Custom layer to compute the L1 distance between the features of the two images\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([feature1, feature2])\n",
    "    \n",
    "    # Calculate similarity score\n",
    "    similarity = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[input1, input2], outputs=similarity)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_13 (Functional)      (None, 1024)         48270144    input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1024)         0           functional_13[0][0]              \n",
      "                                                                 functional_13[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            1025        lambda_4[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 48,271,169\n",
      "Trainable params: 33,556,481\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model((IMG_SIZE[0], IMG_SIZE[1], NUM_CHANNELS))\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr = 0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_logdir = \"logs/scalars/\"\n",
    "for f in os.listdir(base_logdir):\n",
    "    file_path = os.path.join(base_logdir, f)\n",
    "    \n",
    "    shutil.rmtree(file_path)\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 1 classes.\n",
      "Found 100 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.7073 - accuracy: 0.6250 - precision: 0.8571 - recall: 0.5455WARNING:tensorflow:From f:\\work\\university\\year 5\\acs6420_advanced project\\code\\siamese_env\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2/6 [=========>....................] - ETA: 0s - loss: 2.0430 - accuracy: 0.5625 - precision: 0.6087 - recall: 0.7368WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1000s vs `on_train_batch_end` time: 0.2510s). Check your callbacks.\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.9719 - accuracy: 0.5521 - precision: 0.5417 - recall: 0.5532Found 80 images belonging to 1 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "6/6 [==============================] - 19s 3s/step - loss: 1.9719 - accuracy: 0.5521 - precision: 0.5417 - recall: 0.5532 - val_loss: 0.7624 - val_accuracy: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 1.8386 - accuracy: 0.4643 - precision: 0.5098 - recall: 0.5652 - val_loss: 0.7116 - val_accuracy: 0.4875 - val_precision: 0.4875 - val_recall: 1.0000\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 1.2409 - accuracy: 0.5119 - precision: 0.4889 - recall: 0.5500 - val_loss: 0.7225 - val_accuracy: 0.5000 - val_precision: 1.0000 - val_recall: 0.0476\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.8769 - accuracy: 0.5595 - precision: 0.5385 - recall: 0.3590 - val_loss: 0.6509 - val_accuracy: 0.6875 - val_precision: 0.6667 - val_recall: 0.7500\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.7901 - accuracy: 0.6071 - precision: 0.5625 - recall: 0.8780 - val_loss: 0.6910 - val_accuracy: 0.5375 - val_precision: 0.6250 - val_recall: 0.3488\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.7240 - accuracy: 0.6310 - precision: 0.8182 - recall: 0.4000 - val_loss: 0.6700 - val_accuracy: 0.6250 - val_precision: 0.6136 - val_recall: 0.6750\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.8237 - accuracy: 0.5357 - precision: 0.5224 - recall: 0.8333 - val_loss: 0.6650 - val_accuracy: 0.5625 - val_precision: 0.5333 - val_recall: 0.4324\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.6995 - accuracy: 0.6146 - precision: 0.6923 - recall: 0.3830 - val_loss: 0.6717 - val_accuracy: 0.5250 - val_precision: 0.5128 - val_recall: 0.5128\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.7270 - accuracy: 0.5952 - precision: 0.5667 - recall: 0.8095 - val_loss: 0.6757 - val_accuracy: 0.5750 - val_precision: 0.5283 - val_recall: 0.7568\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.6165 - accuracy: 0.6548 - precision: 0.7143 - recall: 0.4878 - val_loss: 0.6738 - val_accuracy: 0.6375 - val_precision: 0.6905 - val_recall: 0.6444\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.7027 - accuracy: 0.6429 - precision: 0.6078 - recall: 0.7561 - val_loss: 0.6490 - val_accuracy: 0.6500 - val_precision: 0.6000 - val_recall: 0.9000\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.4950 - accuracy: 0.7738 - precision: 0.7872 - recall: 0.8043 - val_loss: 0.6492 - val_accuracy: 0.6250 - val_precision: 0.6667 - val_recall: 0.4211\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.5615 - accuracy: 0.6548 - precision: 0.7241 - recall: 0.5000 - val_loss: 0.6359 - val_accuracy: 0.6625 - val_precision: 0.6275 - val_recall: 0.8000\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.6374 - accuracy: 0.6548 - precision: 0.5938 - recall: 0.9268 - val_loss: 0.6938 - val_accuracy: 0.5375 - val_precision: 0.8333 - val_recall: 0.2222\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.7808 - accuracy: 0.6042 - precision: 0.9091 - recall: 0.2128 - val_loss: 0.6357 - val_accuracy: 0.6250 - val_precision: 0.7500 - val_recall: 0.3158\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.7520 - accuracy: 0.5714 - precision: 0.5541 - recall: 0.9318 - val_loss: 0.6144 - val_accuracy: 0.6500 - val_precision: 0.6364 - val_recall: 0.7000\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.5071 - accuracy: 0.7262 - precision: 0.7179 - recall: 0.7000 - val_loss: 0.6069 - val_accuracy: 0.6125 - val_precision: 0.7500 - val_recall: 0.3659\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.5062 - accuracy: 0.7619 - precision: 0.8621 - recall: 0.6098 - val_loss: 0.6001 - val_accuracy: 0.6500 - val_precision: 0.6857 - val_recall: 0.5854\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.4474 - accuracy: 0.8333 - precision: 0.7627 - recall: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.6125 - val_precision: 0.6875 - val_recall: 0.2973\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.4863 - accuracy: 0.7500 - precision: 0.8750 - recall: 0.6222 - val_loss: 0.5609 - val_accuracy: 0.7750 - val_precision: 0.8621 - val_recall: 0.6410\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3384 - accuracy: 0.8810 - precision: 0.8333 - recall: 0.9211 - val_loss: 0.5821 - val_accuracy: 0.6750 - val_precision: 0.7692 - val_recall: 0.5000\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.4285 - accuracy: 0.8646 - precision: 0.8462 - recall: 0.8980 - val_loss: 0.6269 - val_accuracy: 0.6000 - val_precision: 0.6000 - val_recall: 0.4737\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.3474 - accuracy: 0.9048 - precision: 0.9722 - recall: 0.8333 - val_loss: 0.6057 - val_accuracy: 0.6875 - val_precision: 0.8519 - val_recall: 0.5227\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3857 - accuracy: 0.8214 - precision: 0.7674 - recall: 0.8684 - val_loss: 0.5971 - val_accuracy: 0.7125 - val_precision: 0.8077 - val_recall: 0.5385\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.4263 - accuracy: 0.7976 - precision: 0.8378 - recall: 0.7381 - val_loss: 0.5908 - val_accuracy: 0.7250 - val_precision: 0.9048 - val_recall: 0.4872\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.4483 - accuracy: 0.7857 - precision: 0.7400 - recall: 0.8810 - val_loss: 0.5953 - val_accuracy: 0.6750 - val_precision: 0.7500 - val_recall: 0.5250\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3505 - accuracy: 0.8929 - precision: 0.8542 - recall: 0.9535 - val_loss: 0.5965 - val_accuracy: 0.6875 - val_precision: 0.8667 - val_recall: 0.3611\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3734 - accuracy: 0.8571 - precision: 0.8810 - recall: 0.8409 - val_loss: 0.5904 - val_accuracy: 0.6625 - val_precision: 0.7143 - val_recall: 0.5128\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.3843 - accuracy: 0.8438 - precision: 0.8039 - recall: 0.8913 - val_loss: 0.6701 - val_accuracy: 0.6625 - val_precision: 0.9444 - val_recall: 0.3953\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.3571 - accuracy: 0.8929 - precision: 0.8750 - recall: 0.9333 - val_loss: 0.5873 - val_accuracy: 0.7250 - val_precision: 0.7500 - val_recall: 0.6316\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2802 - accuracy: 0.9167 - precision: 0.9459 - recall: 0.8750 - val_loss: 0.6099 - val_accuracy: 0.7250 - val_precision: 0.8462 - val_recall: 0.5500\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3282 - accuracy: 0.8690 - precision: 0.8571 - recall: 0.8780 - val_loss: 0.6983 - val_accuracy: 0.5250 - val_precision: 0.5882 - val_recall: 0.4545\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2696 - accuracy: 0.9524 - precision: 0.9184 - recall: 1.0000 - val_loss: 0.5924 - val_accuracy: 0.7250 - val_precision: 0.9000 - val_recall: 0.4737\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2185 - accuracy: 0.9643 - precision: 0.9500 - recall: 0.9744 - val_loss: 0.5836 - val_accuracy: 0.7375 - val_precision: 1.0000 - val_recall: 0.4474\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3140 - accuracy: 0.9048 - precision: 0.9286 - recall: 0.8864 - val_loss: 0.6063 - val_accuracy: 0.6875 - val_precision: 0.8182 - val_recall: 0.4615\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.3204 - accuracy: 0.8854 - precision: 0.8627 - recall: 0.9167 - val_loss: 0.6549 - val_accuracy: 0.6375 - val_precision: 0.7895 - val_recall: 0.3750\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.3192 - accuracy: 0.9167 - precision: 0.9302 - recall: 0.9091 - val_loss: 0.6398 - val_accuracy: 0.5875 - val_precision: 0.7895 - val_recall: 0.3409\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3299 - accuracy: 0.8810 - precision: 0.8333 - recall: 0.9524 - val_loss: 0.5407 - val_accuracy: 0.7750 - val_precision: 0.8800 - val_recall: 0.5946\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2714 - accuracy: 0.8929 - precision: 0.9375 - recall: 0.8108 - val_loss: 0.6356 - val_accuracy: 0.6625 - val_precision: 0.8333 - val_recall: 0.3846\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2684 - accuracy: 0.9286 - precision: 0.9000 - recall: 0.9783 - val_loss: 0.7014 - val_accuracy: 0.6500 - val_precision: 0.8000 - val_recall: 0.3243\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2196 - accuracy: 0.9524 - precision: 0.9750 - recall: 0.9286 - val_loss: 0.7006 - val_accuracy: 0.6125 - val_precision: 0.9091 - val_recall: 0.2500\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.3087 - accuracy: 0.9048 - precision: 0.8667 - recall: 0.9512 - val_loss: 0.6835 - val_accuracy: 0.6250 - val_precision: 1.0000 - val_recall: 0.2308\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.2174 - accuracy: 0.9583 - precision: 0.9583 - recall: 0.9583 - val_loss: 0.7367 - val_accuracy: 0.6375 - val_precision: 0.8824 - val_recall: 0.3571\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 20s 3s/step - loss: 0.2247 - accuracy: 0.9405 - precision: 0.9091 - recall: 0.9756 - val_loss: 0.7211 - val_accuracy: 0.6250 - val_precision: 0.7368 - val_recall: 0.3590\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2222 - accuracy: 0.9286 - precision: 0.9362 - recall: 0.9362 - val_loss: 0.7954 - val_accuracy: 0.5750 - val_precision: 0.7143 - val_recall: 0.2500\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2206 - accuracy: 0.9048 - precision: 0.9394 - recall: 0.8378 - val_loss: 0.6880 - val_accuracy: 0.6250 - val_precision: 0.7273 - val_recall: 0.4000\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2120 - accuracy: 0.9643 - precision: 0.9318 - recall: 1.0000 - val_loss: 0.7445 - val_accuracy: 0.7000 - val_precision: 0.9130 - val_recall: 0.4884\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2139 - accuracy: 0.9286 - precision: 0.9512 - recall: 0.9070 - val_loss: 0.6385 - val_accuracy: 0.6625 - val_precision: 0.7500 - val_recall: 0.4054\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 18s 3s/step - loss: 0.2507 - accuracy: 0.8929 - precision: 0.9048 - recall: 0.8837 - val_loss: 0.6880 - val_accuracy: 0.6625 - val_precision: 0.8500 - val_recall: 0.4146\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 19s 3s/step - loss: 0.2820 - accuracy: 0.8958 - precision: 0.8421 - recall: 0.9796 - val_loss: 0.5552 - val_accuracy: 0.7000 - val_precision: 0.7667 - val_recall: 0.5750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b8b600ec88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generator(train_dir, 'train'), steps_per_epoch=100//BATCH_SIZE, \n",
    "          validation_data=generator(val_dir, 'val'), validation_steps=80//BATCH_SIZE,\n",
    "#           class_weight={0: 2, 1: 1}, \n",
    "          epochs=50, callbacks=[tensorboard_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9680), started 8:39:57 ago. (Use '!kill 9680' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3199fcd270cc68d7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3199fcd270cc68d7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/scalars --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir2 = val_dir + '/Both'\n",
    "im_correct = []\n",
    "im_incorrect = []\n",
    "\n",
    "# Load in validation data\n",
    "for folder in os.listdir(val_dir2):\n",
    "    if folder == 'Correct Edge':\n",
    "        folder_path = os.path.join(val_dir2, folder)\n",
    "        for im in os.listdir(folder_path):\n",
    "            im_array = cv2.imread(os.path.join(folder_path, im))\n",
    "            im_array = cv2.resize(im_array, IMG_SIZE)\n",
    "            im_array = cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB)\n",
    "            im_array = im_array/255\n",
    "            im_correct.append(im_array)\n",
    "    else:\n",
    "        folder_path = os.path.join(val_dir2, folder)\n",
    "        for im in os.listdir(folder_path):\n",
    "            im_array = cv2.imread(os.path.join(folder_path, im))\n",
    "            im_array = cv2.resize(im_array, IMG_SIZE)\n",
    "            im_array = cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB)\n",
    "            im_array = im_array/255\n",
    "            im_incorrect.append(im_array)\n",
    "            \n",
    "im_correct = np.asarray(im_correct)\n",
    "im_incorrect = np.asarray(im_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_correct = 10  # Number of correct images to compare against\n",
    "\n",
    "predict_correct = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for im in im_correct:\n",
    "    Y = []\n",
    "    idxs = np.random.randint(im_correct.shape[0], size=n_correct)  # Correct images to compare against\n",
    "    \n",
    "    for idx in idxs:\n",
    "        im_ref = im_correct[idx]\n",
    "        im_ref = im_ref.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        im = im.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        y = np.rint(model.predict([im_ref, im]))\n",
    "        Y.append(y)\n",
    "     \n",
    "    vote_same = np.sum(Y)\n",
    "    # If more than half the images compared to get voted as same, then set vote to same, otherwise to different\n",
    "    if vote_same >= n_correct/2:\n",
    "        Y_vote = 1\n",
    "        predict_correct += 1\n",
    "    else:\n",
    "        Y_vote = 0\n",
    "        fn += 1\n",
    "        \n",
    "\n",
    "for im in im_incorrect:\n",
    "    Y = []\n",
    "    idxs = np.random.randint(im_correct.shape[0], size=n_correct)  # Correct images to compare against\n",
    "    \n",
    "    for idx in idxs:\n",
    "        im_ref = im_correct[idx]\n",
    "        im_ref = im_ref.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        im = im.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "        y = np.rint(model.predict([im_ref, im]))\n",
    "        Y.append(y)\n",
    "     \n",
    "    vote_same = np.sum(Y)\n",
    "    # If more than half the images compared to get voted as same, then set vote to same, otherwise to different\n",
    "    if vote_same >= n_correct/2:\n",
    "        Y_vote = 1\n",
    "        fp += 1\n",
    "    else:\n",
    "        Y_vote = 0\n",
    "        predict_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy =  0.7625\n",
      "Validation Precision =  0.9384615384615385\n",
      "Validation Recall =  0.8026315789473685\n"
     ]
    }
   ],
   "source": [
    "Val_acc = predict_correct/80\n",
    "print(\"Validation Accuracy = \", Val_acc)\n",
    "\n",
    "Val_precision = predict_correct/(predict_correct + fp)\n",
    "print(\"Validation Precision = \", Val_precision)\n",
    "\n",
    "Val_recall = predict_correct/(predict_correct + fn)\n",
    "print(\"Validation Recall = \", Val_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-way validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task(N, source):\n",
    "    im_list = []\n",
    "    target_list = []\n",
    "    if source == 'train':\n",
    "        im_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Training/All Cases/Both'\n",
    "    else:\n",
    "        im_dir = 'F:/Work/University/Year 5/ACS6420_Advanced Project/Code/Data/Post Installation/Binary/Validation/All Cases/Both'\n",
    "    \n",
    "    # Get single reference image\n",
    "    reference = rng.choice(os.listdir(im_dir+'/Correct'))\n",
    "    im_reference = cv2.imread(os.path.join(im_dir+'/Correct', reference))\n",
    "    im_reference = cv2.resize(im_reference, IMG_SIZE)\n",
    "    im_reference = cv2.cvtColor(im_reference, cv2.COLOR_BGR2RGB)\n",
    "    im_reference = im_reference/255\n",
    "    im_reference = im_reference.reshape(1,SIZE,SIZE,NUM_CHANNELS)\n",
    "    target_reference = 0\n",
    "        \n",
    "    #plt.imshow(im_reference)\n",
    "    #plt.show()\n",
    "        \n",
    "    # Get N test images\n",
    "    for i in range(N):\n",
    "        if i == 0:\n",
    "            test = rng.choice(os.listdir(im_dir+'/Correct'))\n",
    "            im_test = cv2.imread(os.path.join(im_dir+'/Correct', test))\n",
    "        else:\n",
    "            test = rng.choice(os.listdir(im_dir+'/Incorrect'))\n",
    "            im_test = cv2.imread(os.path.join(im_dir+'/Incorrect', test))\n",
    "\n",
    "        im_test = cv2.resize(im_test, IMG_SIZE)\n",
    "        im_test = cv2.cvtColor(im_test, cv2.COLOR_BGR2RGB)\n",
    "        im_test = im_test/255\n",
    "        im_list.append(im_test)\n",
    "        if i == 0:\n",
    "            target_list.append(0)\n",
    "        else:\n",
    "            target_list.append(1)\n",
    "        \n",
    "    #plt.imshow(im_list[-1])\n",
    "    #plt.show()\n",
    "    \n",
    "    #im_list, target_list = shuffle(im_list, target_list)\n",
    "    \n",
    "    im_array = np.asarray(im_list)\n",
    "    \n",
    "    return im_reference, target_reference, im_array, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_way_validate(model, N, it, source):\n",
    "    n_correct = 0\n",
    "    \n",
    "    for i in range(it):\n",
    "        score_list = []\n",
    "        im_reference, target_reference, im_list, target_list = create_task(N, source)\n",
    "        \n",
    "        for j in range(N):\n",
    "            score = model.predict([im_reference, np.reshape(im_list[j], (1,SIZE,SIZE,NUM_CHANNELS))])\n",
    "            score_list.append(score)\n",
    "            \n",
    "        if np.argmax(score_list) == np.argmin(target_list):\n",
    "            n_correct += 1\n",
    "            \n",
    "    percent_correct = (100*n_correct / it)\n",
    "    \n",
    "    return percent_correct, score_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0\n"
     ]
    }
   ],
   "source": [
    "N_way = 10\n",
    "iterations = 40\n",
    "val_source = 'train'\n",
    "\n",
    "result, score_list = n_way_validate(model, N_way, iterations, val_source)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.5\n"
     ]
    }
   ],
   "source": [
    "val_source = 'validation'\n",
    "\n",
    "result, score_list = n_way_validate(model, N_way, iterations, val_source)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
